---
title: "Correlation and Regression"
output:
  pdf_document
---

# 1. Visualizing two variables

## 1) Visualizing bivariate relationships

Both variables are numerical

Response variable, a.k.a. y, dependent

Explanatory variable: Something you think might be related to the response. a.k.a. x, independent, predictor

Visualizing the relationship:
scatterplot is the easiest one and most commom use graph.
Dependent variable is usually put to y axis, while explanatory variable is put to x axis. 

boxplots: We can think of boxplots as scatterplots but with discretized explnatory variable.
cut() fundtion discretizes. Chose appropriate number of "boxes". 

## 2) Scatterplots
Scatterplots are the most common and effective tools for visualizing the relationship between two numeric variables.

The ncbirths dataset is a random sample of 1,000 cases taken from a larger dataset collected in 2004. Each case describes the birth of a single child born in North Carolina, along with various characteristics of the child (e.g. birth weight, length of gestation, etc.), the child's mother (e.g. age, weight gained during pregnancy, smoking habits, etc.) and the child's father (e.g. age).

```{r}
#install.packages("openintro")
library(openintro)
data(ncbirths)
```

Using the ncbirths dataset, make a scatterplot using ggplot() to illustrate how the birth weight of these babies varies according to the number of weeks of gestation.
```{r}
library(ggplot2)
ggplot(ncbirths, aes(x = weeks, y = weight )) +
  geom_point()
```

## 3) Boxplots as discretized/conditioned scatterplots
If it is helpful, you can think of boxplots as scatterplots for which the variable on the x-axis has been discretized.

The cut() function takes two arguments: the continuous variable you want to discretize and the number of breaks that you want to make in that continuous variable in order to discretize it.

Using the ncbirths dataset again, make a boxplot illustrating how the birth weight of these babies varies according to the number of weeks of gestation. This time, use the cut() function to discretize the x-variable into six intervals (i.e. five breaks).
```{r}
ggplot(ncbirths, aes(x = cut(weeks, breaks = 5), y = weight)) +
 geom_boxplot()
```

## 4) Charaterizing scatterplots
Form (e.g. linear, quadratic, non-linear
Direction (e.g. positive, negative))
Strength (how much scatter/noise?)
Outliers

(Let's google scatterplot and practice to judge the relationship by looking at those examples.)

Creating scatterplots is simple and they are so useful that is it worthwhile to expose yourself to many examples. Over time, you will gain familiarity with the types of patterns that you see. You will begin to recognize how scatterplots can reveal the nature of the relationship between two variables.

Throughout this chapter, we will be using several datasets listed below. These data are available through the openintro package. Briefly:

The mammals dataset contains information about 39 different species of mammals, including their body weight, brain weight, gestation time, and a few other variables.
The mlbBat10 dataset contains batting statistics for 1,199 Major League Baseball players during the 2010 season.
The bdims dataset contains body girth and skeletal diameter measurements for 507 physically active individuals.
The smoking dataset contains information on the smoking habits of 1,691 citizens of the United Kingdom.
To see more thorough documentation, use the ? or help() functions.

Using the mammals dataset, create a scatterplot illustrating how the brain weight of a mammal varies as a function of its body weight.
```{r}
library(openintro)
data(mammals)
str(mammals)
ggplot(mammals, aes(x = body_wt, y = brain_wt)) +
  geom_point()
```

Using the mlbBat10 dataset, create a scatterplot illustrating how the slugging percentage (SLG) of a player varies as a function of his on-base percentage (OBP).
```{r}
library(openintro)
data(mlbbat10)
str(mlbbat10)

ggplot(mlbbat10, aes(x = obp , y = slg )) +
  geom_point()
```

Using the bdims dataset, create a scatterplot illustrating how a person's weight varies as a function of their height. Use color to separate by sex, which you'll need to coerce to a factor with factor().
```{r}
data(bdims)
str(bdims)
?bdlims

ggplot(bdims, aes(x = hgt, y = wgt, col = factor(sex)))+
  geom_point()
```

Using the smoking dataset, create a scatterplot illustrating how the amount that a person smokes on weekdays varies as a function of their age.
```{r}
data(smoking)
str(smoking)
?smoking

ggplot(smoking, aes(x = age, y = amt_weekdays)) +
  geom_point()
```


## 5) Transformations
The relationship between two variables may not be linear. In these cases we can sometimes see strange and even inscrutable patterns in a scatterplot of the data. Sometimes there really is no meaningful relationship between the two variables. Other times, a careful transformation of one or both of the variables can reveal a clear relationship.

Recall the bizarre pattern that you saw in the scatterplot between brain weight and body weight among mammals in a previous exercise. Can we use transformations to clarify this relationship?

ggplot2 provides several different mechanisms for viewing transformed relationships. The coord_trans() function transforms the coordinates of the plot. Alternatively, the scale_x_log10() and scale_y_log10() functions perform a base-10 log transformation of each axis. Note the differences in the appearance of the axes.

With mammals dataset, 

Use coord_trans() to create a scatterplot showing how a mammal's brain weight varies as a function of its body weight, where both the x and y axes are on a "log10" scale.
```{r}
ggplot(mammals, aes(x = body_wt, y = brain_wt)) +
  geom_point() +
  coord_trans(x = "log10", y = "log10")
```

Use scale_x_log10() and scale_y_log10() to achieve the same effect but with different axis labels and grid lines.
```{r}
ggplot(mammals, aes(x = body_wt, y = brain_wt)) +
  geom_point() + 
  scale_x_log10() +
  scale_y_log10()
```

## 6) Outliers
```{r}
data(mlbbat10)
str(mlbbat10)

ggplot(mlbbat10, aes(x = stolen_base , y = home_run )) +
  geom_point()
```
Because values are integer, there must be overlapped points. To clarify it, let's use alpha, and position = "jitter".
https://ggplot2.tidyverse.org/reference/geom_jitter.html

```{r}
data(mlbbat10)
str(mlbbat10)

ggplot(mlbbat10, aes(x = stolen_base , y = home_run )) +
  geom_point(alpha = 0.5, position = "jitter")
```
We can observe two outliers. In later section, we will discuss how outliers can affect the results of a linear regression model and how we can deal with them. For now, it is enough to simply identify them and note how the relationship between two variables may change as a result of removing outliers.

Recall that in the baseball example earlier in the chapter, most of the points were clustered in the lower left corner of the plot, making it difficult to see the general pattern of the majority of the data. This difficulty was caused by a few outlying players whose on-base percentages (obp) were exceptionally high. These values are present in our dataset only because these players had very few batting opportunities.

Both obp and slg are known as rate statistics, since they measure the frequency of certain events (as opposed to their count). In order to compare these rates sensibly, it makes sense to include only players with a reasonable number of opportunities, so that these observed rates have the chance to approach their long-run frequencies.

In Major League Baseball, batters qualify for the batting title only if they have 3.1 plate appearances per game. This translates into roughly 502 plate appearances in a 162-game season. The mlbBat10 dataset does not include plate appearances as a variable, but we can use at-bats (at_bat) -- which constitute a subset of plate appearances -- as a proxy.

Use filter() to create a scatterplot for SLG as a function of OBP among players who had at least 200 at-bats.
```{r}
library(dplyr)
mlbbat10 %>%
  filter(at_bat >= 200) %>% 
  ggplot(aes(x = obp, y = slg)) +
  geom_point()
```
Let's compare the result with this:
```{r}
mlbbat10 %>%
  ggplot(aes(x = obp, y = slg)) +
  geom_point()
```

And find the row of mlbBat10 corresponding to the one player with at least 200 at-bats whose OBP was below 0.200.
```{r}
mlbbat10 %>%
  filter(at_bat >= 200, obp < 0.200)
```

# 2. Correlation

## 1) Quantifying the strength of bivariate relationships

Correlation is the way to quantify the strength of relationship. 
Correlation coefficient between -1 and 1
Sign --> direction
Magnitude --> strength (around 0.5 moderate, around 0.2 week) ONLY WHEN the relationship is linear.
There are cases that have low correlation but non-linear strong relationship. 
```{r}

ncbirths %>%
  ggplot(aes(x = weeks, y = weight, col = habit)) +
  geom_point()

```
We can observe that age and pace is highly related but not linear way. 

Pearson product-moment correlation: 
$$ r(x,y) = \frac{Cov(x,y)}{\sqrt{S_{XX}*S_{YY}}}$$
$$ r(x,y) = \frac{\sum^n_{i = 1}(x_i- \bar x)(y_i- \bar y)}{\sum^n_i(x_i- \bar x)^2 \cdot \sum^n_i(y_i- \bar y)^2} $$
mathmatic equation may reinforce your intuition.

Which correlation is invalid among 0.15, 1.2, -0.9?
Which correlation is the strongest among 0.6, 0.2, -0.8?


## 2) Computung correlation
The cor(x, y) function will compute the Pearson product-moment correlation between variables, x and y. Since this quantity is symmetric with respect to x and y, it doesn't matter in which order you put the variables.

At the same time, the cor() function is very conservative when it encounters missing data (e.g. NAs). The use argument allows you to override the default behavior of returning NA whenever any of the values encountered is NA. Setting the use argument to "pairwise.complete.obs" allows cor() to compute the correlation coefficient for those observations where the values of x and y are both not missing.

Use cor() to compute the correlation between the birthweight of babies in the ncbirths dataset and their mother's age. There is no missing data in either variable.
```{r}
ncbirths %>% 
  summarize(N = n(), r = cor(weight, mage))
```
Compute the correlation between the birthweight and the number of weeks of gestation for all non-missing pairs.
```{r}
ncbirths %>% 
  summarize(N = n(), r = cor(weight, weeks, use = "pairwise.complete.obs"))
```

## 3) The Anscombe dataset
Syntheric data: so many identical carateristics but different relationship.
Visit here: https://en.wikipedia.org/wiki/Anscombe%27s_quartet

In 1973, Francis Anscombe famously created four datasets with remarkably similar numerical properties (mean of x, mean of y, observations, standard deviation of x and y, correlation between x and y), but obviously different graphic relationships. The Anscombe dataset contains the x and y coordinates for these four datasets, along with a grouping variable, set, that distinguishes the quartet.

It may be helpful to remind yourself of the graphic relationship by viewing the four scatterplots:


```{r}
data(anscombe)
?anscombe

anscombe %>%
  ggplot(aes(x = x1, y = y1)) +
  geom_point()
```

```{r}
anscombe %>%
  ggplot(aes(x = x2, y = y2)) +
  geom_point()
```
```{r}
anscombe %>%
  ggplot(aes(x = x3, y = y3)) +
  geom_point()
```
We can observe one outlier. 

```{r}
anscombe %>%
  ggplot(aes(x = x4, y = y4)) +
  geom_point()
```
Here again, outlier. 


## 5) Perception of correlation
https://statistics.stanford.edu/sites/g/files/sbiybj6031/f/EFS%20NSF%20206.pdf
Estimating the value of the correlation coefficient between two quantities from their scatterplot can be tricky. Statisticians have shown that people's perception of the strength of these relationships can be influenced by design choices like the x and y scales.

Nevertheless, with some practice your perception of correlation will improve. Toggle through the four scatterplots, each of which you've seen in a previous exercise. Jot down your best estimate of the value of the correlation coefficient between each pair of variables. Then, compare these values to the actual values you compute in this exercise.

Each graph in the plotting window corresponds to an instruction below. Compute the correlation between...
OBP and SLG for all players in the mlbBat10 dataset.
OBP and SLG for all players in the mlbBat10 dataset with at least 200 at-bats.
Height and weight for each sex in the bdims dataset.
Body weight and brain weight for all species of mammals. Alongside this computation, compute the correlation between the same two quantities after taking their natural logarithms.
```{r}
# Correlation for all baseball players
mlbbat10%>%
  summarize(N = n(), r = cor(obp, slg)) 


# Correlation for all players with at least 200 ABs
mlbbat10 %>%
  filter(at_bat >= 200) %>%
  summarize(N = n(), r = cor(obp, slg))

# Correlation of body dimensions
bdims %>%
  group_by(sex) %>%
  summarize(N = n(), r = cor(hgt, wgt))

# Correlation among mammals, with and without log
mammals %>%
  summarize(N = n(), 
            r = cor(body_wt, brain_wt), 
            r_log = cor(log(body_wt), log(brain_wt)))
```

## 6) Interpreting correlation in context
correlation does not always means causation.

https://www.nytimes.com/2012/11/02/business/questions-raised-on-withdrawal-of-congressional-research-services-report-on-tax-rates.html

## 7) Correlation and casation
Question: In the San Francisco Bay Area from 1960-1967, the correlation between the birthweight of 1,236 babies and the length of their gestational period was 0.408. Which of the following conclusions is not a valid statistical interpretation of these results.
1. We observed that babies with longer gestational periods tended to be heavier at birth.
2. It may be that a longer gestational period contributes to a heavier birthweight among babies, but a randomized, controlled experiment is needed to confirm this observation.
3. Staying in the womb longer causes babies to be heavier when they are born.
4. These data suggest that babies with longer gestational periods tend to be heavier at birth, but there are many potential confounding factors that were not taken into account.


## 8) Spurious correlation
Please see the ppt slides for the examples of spurious correlation

# 3. Simple linear regression

## 1) Visualization of linear models
```{r}
#install.packages("openintro")
library(openintro)
```

```{r}
data(possum)
str(possum)
```

```{r}

ggplot(data=possum, aes(y = total_l, x = tail_l)) +
  geom_point()
```

```{r}
ggplot(data=possum, aes(y = total_l, x = tail_l)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 2.5)
```

```{r}
ggplot(data=possum, aes(y = total_l, x = tail_l)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 2.5)
# It doesn't capture the general trend that we can imagine
```

```{r}
ggplot(data=possum, aes(y = total_l, x = tail_l)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 2.3)

```

```{r}
#Not through the origin
ggplot(data=possum, aes(y = total_l, x = tail_l)) +
  geom_point() +
  geom_abline(intercept = 40, slope = 1.3)
```
How can we draw a line with the best fit? How can we judge whether the line is the best fit or not?

```{r}
ggplot(data=possum, aes(y = total_l, x = tail_l)) +
  geom_point() +
  geom_smooth(method = "lm") # linear model
# gray area represents the standard error of the line
# we can remove it by adding se = False

```
## 2) The "best fit" line
The simple linear regression model for a numeric response as a function of a numeric explanatory variable can be visualized on the corresponding scatterplot by a straight line. This is a "best fit" line that cuts through the data in a way that minimizes the distance between the line and the data points.

We might consider linear regression to be a specific example of a larger class of smooth models. The geom_smooth() function allows you to draw such models over a scatterplot of the data itself. This technique is known as visualizing the model in the data space. The method argument to geom_smooth() allows you to specify what class of smooth model you want to see. Since we are exploring linear models, we'll set this argument to the value "lm".

Note that geom_smooth() also takes an se argument that controls the standard error, which we will ignore for now.

Create a scatterplot of body weight as a function of height for all individuals in the bdims dataset with a simple linear model plotted over the data.

```{r}
ggplot(bdims, aes(x = hgt, y = wgt)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

## 3) Uniqueness of least square regression line
The least squares criterion implies that the slope of the regression line is unique. 

## 4) Understanding regression model

### Generic statistical model: 

$$response = f(explanatory) + noise$$
$$response = intercept + (slope * explanatory) + noise$$

### Regression model:

$$Y = \beta_0 + \beta_1 \cdot X + \epsilon,$$    
$$\epsilon \sim N(0, \sigma_\epsilon)$$

### Fitted value:

$$ \hat Y = \hat \beta_0 + \hat \beta_1 \cdot X$$

### Residual:

$$e = Y - \hat Y$$

### Fitting procedure:

Given n observation pairs $(x_i, y_i)$...

Find $\hat\beta_0, \hat \beta_1$ that minimize $\sum ^n _{i=1} e^2_i$ 


### Least squares:

Easy, deterministic, unique solution

Residuals sum to zero

Line must pass through $$(\bar x, \bar y)$$

Other criteria exist but not in this course


### Key concept:

$\hat Y$ is expected value given corresponding X

$\hat \beta$ are estimates of true, unkown $\beta$s

Regiduals (e's) are estimates of true, unknown $\epsilon$

"Error" may be misleading term, better: noise

## 5) Regression model output terminology
The fitted model for the poverty rate of U.S. counties as a function of high school graduation rate is:
$$\hat{poverty} = 64.594 - 0.591 \cdot hsgrad$$
In Hampshire County in western Massachusetts, the high school graduation rate is 92.4%. These two facts imply that the poverty rate in Hampshire County is ___.

1. exactly 11.7%  2. exactly 10.0%  3. expected to be 10.0%    4. exected to be 11.7%


## 6) Regression to the mean
"Regression to the mean" is a concept attributed to Sir Francis Galton. The basic idea is that extreme random observations will tend to be less extreme upon a second trial. This is simply due to chance alone. While "regression to the mean" and "linear regression" are not the same thing, we will examine them together in this exercise.

One way to see the effects of regression to the mean is to compare the heights of parents to their children's heights. While it is true that tall mothers and fathers tend to have tall children, those children tend to be less tall than their parents, relative to average. That is, fathers who are 3 inches taller than the average father tend to have children who may be taller than average, but by less than 3 inches.

The Galton_men and Galton_women datasets contain data originally collected by Galton himself in the 1880s on the heights of men and women, respectively, along with their parents' heights.
```{r}
#install.packages("HistData")
library(HistData)
data(GaltonFamilies)
```



## 7) Regression in parlance of our time
https://www.nytimes.com/2015/03/22/opinion/sunday/seth-stephens-davidowitz-just-how-nepotistic-are-we.html
The author is arguing that...

1. Because of regression to the mean, an outstanding basketball player is likely to have sons that are as good at basketball as him.

2. Because of regression to the mean, an outstanding basketball player is likely to have sons that are not good at basketball.

3. Because of regression to the mean, an outstanding basketball player is likely to have sons that are good at basketball, but not as good as him.

4. Linear regression is incapable of evaluating musical or athletic talent.

# 4. Interpreting regression model

## 1) Interpretation of regression coefficients
```{r}
library(readr)
data("textbooks")
str(textbooks)
# Is textbooks overpriced?

#compare to course number
textbooks %>%
  mutate(course_number = readr::parse_number(as.character(course))) %>%
  ggplot(aes(x = course_number, y = ucla_new)) +
  geom_point()
```

```{r}
# Compare to Amazon?
ggplot(data = textbooks, aes(x = amaz_new, y = ucla_new)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```
```{r}
lm(ucla_new ~ amaz_new, data = textbooks)
# Let's interpretate the result together! 
```

## 2) Fitting simple linear models
While the geom_smooth(method = "lm") function is useful for drawing linear models on a scatterplot, it doesn't actually return the characteristics of the model. As suggested by that syntax, however, the function that creates linear models is lm(). This function generally takes two arguments:
```{r}
?lm
```

A formula that specifies the model. A data argument for the data frame that contains the data you want to use to fit the model.
The lm() function return a model object having class "lm". This object contains lots of information about your regression model, including the data used to fit the model, the specification of the model, the fitted values and residuals, etc.

Using the bdims dataset, create a linear model for the weight of people as a function of their height.
```{r}
lm(wgt~hgt, data = bdims)
```

## 3) Unit and scale
In the previous examples, we fit two regression models:
$$\hat{wgt} = -105.011 + 1.018 \cdot hgt $$
and
$$\hat{SLG} = 0.009 + 1.110 \cdot OBP$$
Which of the following statement is incorrect?
1. A person who is 170 cm tall is expected to weigh about 68 kg.
2. Because the slope coefficient for OBP is larger (1.110) than the slope coefficient for hgt (1.018), we can conclude that the association between OBP and SLG is stronger than the association between height and weight.

## 4) The lm summary output
An "lm" object contains a host of information about the regression model that you fit. There are various ways of extracting different pieces of information.

The coef() function displays only the values of the coefficients. Conversely, the summary() function displays not only that information, but a bunch of other information, including the associated standard error and p-value for each coefficient, the R2, adjusted R2, and the residual standard error. The summary of an "lm" object in R is very similar to the output you would see in other statistical computing environments (e.g. Stata, SPSS, etc.)
```{r}
mod <- lm(wgt ~ hgt, data = bdims)
coef(mod)
```

```{r}
summary(mod)
```

## 5) Fitted value and residuals
Once you have fit a regression model, you are often interested in the fitted values $\hat y_i$ and the residuals $e_i$, where i indexes the observations. Recall that:
$$e_i = y_i - \hat y_i$$
The least squares fitting procedure guarantees that the mean of the residuals is zero (n.b., numerical instability may result in the computed values not being exactly zero). At the same time, the mean of the fitted values must equal the mean of the response variable.

In this exercise, we will confirm these two mathematical facts by accessing the fitted values and residuals with the fitted.values() and residuals() functions, respectively, for the following model:
```{r}
mod <- lm(wgt ~ hgt, data = bdims)

#Confirm that the mean of the body weights equals the mean of the fitted values of mod
mean(bdims$wgt) == mean(fitted.values(mod))

#Compute the mean of the residuals of mod
mean(residuals(mod))
```


## 6) Tidying your linear model
As you fit a regression model, there are some quantities (e.g. $R^2$) that apply to the model as a whole, while others apply to each observation (e.g. $\hat y_i$). If there are several of these per-observation quantities, it is sometimes convenient to attach them to the original data as new variables.

The augment() function from the broom package does exactly this. It takes a model object as an argument and returns a data frame that contains the data on which the model was fit, along with several quantities specific to the regression model, including the fitted values, residuals, leverage scores, and standardized residuals.

```{r}
#install.packages("tidyverse")

# Load the broom package
library(broom)

# Create a new data frame called bdims_tidy that is the augmentation of the mod linear model.
bdims_tidy <- augment(mod)

# View the bdims_tidy data frame using glimpse()
glimpse(bdims_tidy)
```

## 7) Using linear model

Is the text book overpriced? Let's make a prediction
```{r}
mod <- lm(ucla_new ~ amaz_new, data = textbooks)
new_data <- data.frame(amaz_new = 8.49)
isrs <- broom::augment(mod, newdata = new_data)
ggplot(data = textbooks, aes(x = amaz_new, y = ucla_new)) +
  geom_point() + geom_smooth(method = "lm") +
  geom_point(data = isrs, aes(y = .fitted), size = 3, color = "red")
```


The fitted.values() function or the augment()-ed data frame provides us with the fitted values for the observations that were in the original data. However, once we have fit the model, we may want to compute expected values for observations that were NOT present in the data on which the model was fit. These types of predictions are called out-of-sample.

The ben data frame contains a height and weight observation for one person. The mod object contains the fitted model for weight as a function of height for the observations in the bdims dataset. We can use the predict() function to generate expected values for the weight of new individuals. We must pass the data frame of new observations through the newdata argument.
```{r}
ben <- data.frame(wgt = 74.8, hgt = 182.8)
mod <- lm(wgt~hgt, data = bdims)
predict(mod, newdata = ben)
```

## 8) Adding a regression line to a plot manually
The geom_smooth() function makes it easy to add a simple linear regression line to a scatterplot of the corresponding variables. And in fact, there are more complicated regression models that can be visualized in the data space with geom_smooth(). However, there may still be times when we will want to add regression lines to our scatterplot manually. To do this, we will use the geom_abline() function, which takes slope and intercept arguments. Naturally, we have to compute those values ahead of time, but we already saw how to do this (e.g. using coef()).

The coefs data frame contains the model estimates retrieved from coef(). Passing this to geom_abline() as the data argument will enable you to draw a straight line on your scatterplot.
```{r}
# Use geom_abline() to add a line defined in the coefs data frame to a scatterplot of weight vs. height for individuals in the bdims dataset.
mod <- lm(wgt ~ hgt, data = bdims)
coefs <- mod$coefficients
ggplot(data = bdims, aes(x = hgt, y = wgt)) + 
  geom_point() + 
  geom_abline(aes(intercept = coefs[1], slope = coefs[2]),  
              color = "dodgerblue")
```

# 5. Assessing model fit
How can we measure the quality of model fit?

SSE: Sum of Squared Error (RSS, the Residual Sum of Squares; SSR, the Sum of Squared Residuals)
RMSE: the Residual Stadard Error

## 1) Standard error of residuals
One way to assess strength of fit is to consider how far off the model is for a typical case. That is, for some observations, the fitted value will be very close to the actual value, while for others it will not. The magnitude of a typical residual can give us a sense of generally how close our estimates are.

However, recall that some of the residuals are positive, while others are negative. In fact, it is guaranteed by the least squares fitting procedure that the mean of the residuals is zero. Thus, it makes more sense to compute the square root of the mean squared residual, or root mean squared error (RMSE). R calls this quantity the residual standard error.

To make this estimate unbiased, you have to divide the sum of the squared residuals by the degrees of freedom in the model. Thus,
$$RMSE = \sqrt{\frac{\sum_i e^2_i}{d.f.}} = \sqrt{\frac{SSE}{(n-2)}}$$

```{r}
mod <- lm(wgt~hgt, data = bdims)
# View summary of model
summary(mod)

# Compute the mean of the residuals
mean(residuals(mod))

# Compute RMSE using residula() df.residual()
sqrt(sum(residuals(mod)^2) / df.residual(mod))
```

## 2) Assessing simple linear model fit

Only with RMSE, it is difficult to judge whether the model fits well or not.
```{r}
ggplot(data = textbooks, aes(x = amaz_new,y = ucla_new )) + 
  geom_point() +
  geom_smooth(method = "lm", se=FALSE)

ggplot(data=possum, aes(y = total_l, x =tail_l)) +
    geom_point() +
  geom_smooth(method = "lm", se=FALSE)
```
Instead of RMSE, we can look at $R^2$ to judge the model fit.

Null model:
$$\hat y = \bar y$$
using this null model, we can calculate SST.

SSE is $Var(e)$
```{r}
mod_possum<- lm(total_l~ tail_l, data = possum)
mod_possum %>%
  augment() %>%
  summarize(SSE = sum(.resid^2))
```


Recall that the coefficient of determination ($R^2$), can be computed as
$$R^2 = 1 - \frac{SSE}{SST} = 1 - \frac{Var(e)}{Var(y)}$$
where $e$ is the vector of residuals and $y$ is the response variable. This gives us the interpretation of $R^2$ as the percentage of the variability in the response that is explained by the model, since the residuals are the part of that variability that remains unexplained by the model.

## 3) Interpretation of $R^2$


The $R2$ reported for the regression model for poverty rate of U.S. counties in terms of high school graduation rate is 0.464.
```{r}
library(usdata)
data("county_complete")
lm(formula = poverty_2017   ~ hs_grad_2017, data = county_complete) %>%
  summary()
```
How should this result be interpreted?

1. 40.76% of the variability in high school graduate rate among U.S. counties can be explained by poverty rate.

2. 40.76% of the variability in poverty rate among U.S. counties can be explained by high school graduation rate.

3. This model is 40.76% effective.

4. The correlation between poverty rate and high school graduation rate in 2017 is 0.4076.

In the single regression model correation is equal to $R^2$. But correlation is just for the relationship two variable, while $R^2$ is used in broader cases.

## 4) Unusual points
```{r}
regulars <- mlbbat10 %>%
  filter(at_bat >400)
ggplot(data = regulars, aes(x= stolen_base, y = home_run)) +
  geom_point() +
  geom_smooth(method = "lm", se = 0)
```

## 5) Leverage

The leverage of an observation in a regression model is defined entirely in terms of the distance of that observation from the mean of the explanatory variable. That is, observations close to the mean of the explanatory variable have low leverage, while observations far from the mean of the explanatory variable have high leverage. Points of high leverage may or may not be influential.
$$h_i = \frac{1}{n} + \frac{(x_i-\bar x)^2}{\sum^n_{i=1}(x_i-\bar x)^2}$$
This function means that the leverage is the function of distance between explanatory variable and mean of explanatory variable. (It doesn't care about dependent variable!!) 

```{r}
regulars <- mlbbat10 %>%
  filter(at_bat >400)

library(broom)
mod <- lm(home_run~stolen_base, data = regulars)
mod %>%
  augment() %>%
  arrange(desc(.hat)) %>%
  head()

```
```{r}
who <- mlbbat10 %>% 
  filter(home_run == 1, stolen_base == 68)
who2 <- mlbbat10 %>%
  filter(home_run == 2, stolen_base ==52)
who
who2
```

```{r}
which(regulars$name == "J Pierre")
regulars_noLeverage <- regulars[-which(regulars$name == "J Pierre"),]
```

```{r}
mod2 <- lm(home_run~stolen_base, data = regulars_noLeverage)
summary(mod2)
```

Let's compare these two model. 
```{r}
summary(mod)
```
It is diffucult to observe the big change in terms of coefficient. 

## 6) Influence

As noted previously, observations of high leverage may or may not be influential. The influence of an observation depends not only on its leverage, but also on the magnitude of its residual. Recall that while leverage only takes into account the explanatory variable (x), the residual depends on the response variable ($y$) and the fitted value ($\hat y$).

Influential points are likely to have high leverage and deviate from the general relationship between the two variables. We measure influence using Cook's distance, which incorporates both the leverage and residual of each observation.

```{r}
mod <- lm(home_run~stolen_base, data = regulars)

mod %>%
  augment() %>%
  arrange(desc(.cooksd))%>%
  head()
```

```{r}
who3 <- mlbbat10 %>%
  filter(home_run == 54, stolen_base ==9)
who3
```

```{r}
regular_noInfluential <- regulars[-which(regulars$name == "J Bautista"), ]
mod3 <- lm(home_run~stolen_base, data = regular_noInfluential)
summary(mod3)
```
Can you see the difference between model 1 and 3?

## 7) Dealing with outliers
You need to decide whether remove the ouliers or not. 

what is the justification? Be skeptical. 
How does the scope of inference change?

Observations can be outliers for a number of different reasons. Statisticians must always be careful and more importantly, transparent, when dealing with outliers. Sometimes, a better model fit can be achieved by simply removing outliers and re-fitting the model. However, one must have strong justification for doing this. A desire to have a higher $R^2 is not a good enough reason.


